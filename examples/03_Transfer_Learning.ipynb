{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "Datasets computed at high levels of theory are expensive and thus, usually small. \n",
    "A model trained on this data might not be able to generalize well to unseen configurations.\n",
    "Sometimes this can be remedied with transfer learning:\n",
    "By first training a model on a lot of data from a less expensive level of theory, only small adjustments to the parameters are required to accurately reproduce the potential energy surface of a different level of theory.\n",
    "\n",
    "\n",
    "Alternatively, the level of theory might not change, but the dataset is extended.\n",
    "This is the case in learning on the fly scenarios.\n",
    "For a demonstration of using transfer learning for learning on the fly, see the corresponding example from the [IPSuite documentation](https://ipsuite.readthedocs.io/en/latest/).\n",
    "\n",
    "\n",
    "apax comes with discriminative transfer learning capabilities out of the box.\n",
    "In this tutorial we are going to fine tune a model trained on benzene data at the DFT level of theory to CCSDT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "First download the appropriate dataset from the sgdml website.\n",
    "\n",
    "\n",
    "Transfer learning can be facilitated in apax by adding the path to a pre-trained model in the config.\n",
    "Furthermore, we can freeze or reduce the learning rate of various components by adjusting the `optimizer` section of the config.\n",
    "\n",
    "```yaml\n",
    "optimizer:\n",
    "    nn_lr: 0.004\n",
    "    embedding_lr: 0.0\n",
    "```\n",
    "\n",
    "Learning rates of 0.0 will mask the respective weights during training steps.\n",
    "Here, we will freeze the descriptor, reinitialize the scaling and shifting parameters and reduce the learning rate of all other components.\n",
    "\n",
    "We can now fine tune the model by running\n",
    "`apax train config.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import yaml\n",
    "\n",
    "from apax.utils.datasets import (\n",
    "    download_benzene_DFT,\n",
    "    download_md22_benzene_CCSDT,\n",
    "    mod_md_datasets,\n",
    ")\n",
    "from apax.utils.helpers import mod_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire Datasets\n",
    "\n",
    "For this demonstration we will use the DFT and CC versions of the benzene MD17 dataset.\n",
    "We start by downloading both and saving them in an appropriate format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download DFT Data\n",
    "\n",
    "data_path = Path(\"project\")\n",
    "dft_file_path = download_benzene_DFT(data_path)\n",
    "dft_file_path = mod_md_datasets(dft_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download CCSD(T) Data\n",
    "\n",
    "data_path = Path(\"project\")\n",
    "cc_file_path, _ = download_md22_benzene_CCSDT(data_path)\n",
    "cc_file_path = mod_md_datasets(cc_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrain Model\n",
    "\n",
    "First, we will train a model on the \"large\" (in relative terms) but less accurate DFT dataset.\n",
    "A standard model with default optimizers will do just fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ms/miniconda3/envs/apax311/lib/python3.11/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    }
   ],
   "source": [
    "!apax template train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(\"config.yaml\")\n",
    "\n",
    "config_updates = {\n",
    "    \"n_epochs\": 100,\n",
    "    \"data\": {\n",
    "        \"n_train\": 1000,\n",
    "        \"n_valid\": 200,\n",
    "        \"batch_size\": 8,\n",
    "        \"valid_batch_size\": 100,\n",
    "        \"experiment\": \"benzene_dft\",\n",
    "        \"directory\": \"project/models\",\n",
    "        \"data_path\": str(dft_file_path),\n",
    "        \"energy_unit\": \"kcal/mol\",\n",
    "        \"pos_unit\": \"Ang\",\n",
    "    },\n",
    "}\n",
    "config_dict = mod_config(config_path, config_updates)\n",
    "\n",
    "with open(\"config.yaml\", \"w\") as conf:\n",
    "    yaml.dump(config_dict, conf, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO | 16:25:57 | Running on [cuda(id=0)]\n",
      "INFO | 16:25:57 | Initializing Callbacks\n",
      "INFO | 16:25:57 | Initializing Loss Function\n",
      "INFO | 16:25:57 | Initializing Metrics\n",
      "INFO | 16:25:57 | Running Input Pipeline\n",
      "INFO | 16:25:57 | Read data file project/benzene_mod.xyz\n",
      "INFO | 16:25:57 | Loading data from project/benzene_mod.xyz\n",
      "INFO | 16:26:06 | Computing per element energy regression.\n",
      "INFO | 16:26:06 | Initializing Model\n",
      "INFO | 16:26:06 | initializing 1 models\n",
      "INFO | 16:26:10 | Initializing Optimizer\n",
      "INFO | 16:26:10 | Beginning Training\n",
      "Epochs:   0%|                                                               | 0/100 [00:00<?, ?it/s]WARNING | 16:26:23 | SaveArgs.aggregate is deprecated, please use custom TypeHandler (https://orbax.readthedocs.io/en/latest/custom_handlers.html#typehandler) or contact Orbax team to migrate before May 1st, 2024. If your Pytree has empty ([], {}, None) values then use PyTreeCheckpointHandler(..., write_tree_metadata=True, ...) or use StandardCheckpointHandler to avoid TypeHandler Registry error. Please note that PyTreeCheckpointHandler.write_tree_metadata default value is already set to True.\n",
      "Epochs: 100%|███████████████████████████████████| 100/100 [00:36<00:00,  2.71it/s, val_loss=0.00997]\n",
      "INFO | 16:26:47 | Finished training\n"
     ]
    }
   ],
   "source": [
    "!apax train config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline CC Training\n",
    "\n",
    "Next, we require a CC baseline to quantify the effect of pretraining.\n",
    "As with the DFT dataset, we will only use a small fraction of the data to emphasize the effects in the low-data regime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(\"config.yaml\")\n",
    "\n",
    "config_updates = {\n",
    "    \"n_epochs\": 100,\n",
    "    \"data\": {\n",
    "        \"n_train\": 50,\n",
    "        \"n_valid\": 10,\n",
    "        \"batch_size\": 4,\n",
    "        \"valid_batch_size\": 10,\n",
    "        \"experiment\": \"benzene_cc_baseline\",\n",
    "        \"directory\": \"project/models\",\n",
    "        \"data_path\": str(cc_file_path),\n",
    "        \"energy_unit\": \"kcal/mol\",\n",
    "        \"pos_unit\": \"Ang\",\n",
    "    },\n",
    "}\n",
    "config_dict = mod_config(config_path, config_updates)\n",
    "\n",
    "with open(\"config_cc_baseline.yaml\", \"w\") as conf:\n",
    "    yaml.dump(config_dict, conf, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO | 16:26:51 | Running on [cuda(id=0)]\n",
      "INFO | 16:26:51 | Initializing Callbacks\n",
      "INFO | 16:26:51 | Initializing Loss Function\n",
      "INFO | 16:26:51 | Initializing Metrics\n",
      "INFO | 16:26:51 | Running Input Pipeline\n",
      "INFO | 16:26:51 | Read data file project/benzene_ccsd_t-train_mod.xyz\n",
      "INFO | 16:26:51 | Loading data from project/benzene_ccsd_t-train_mod.xyz\n",
      "INFO | 16:26:51 | Computing per element energy regression.\n",
      "INFO | 16:26:51 | Initializing Model\n",
      "INFO | 16:26:51 | initializing 1 models\n",
      "INFO | 16:26:55 | Initializing Optimizer\n",
      "INFO | 16:26:55 | Beginning Training\n",
      "Epochs:   0%|                                                               | 0/100 [00:00<?, ?it/s]WARNING | 16:27:07 | SaveArgs.aggregate is deprecated, please use custom TypeHandler (https://orbax.readthedocs.io/en/latest/custom_handlers.html#typehandler) or contact Orbax team to migrate before May 1st, 2024. If your Pytree has empty ([], {}, None) values then use PyTreeCheckpointHandler(..., write_tree_metadata=True, ...) or use StandardCheckpointHandler to avoid TypeHandler Registry error. Please note that PyTreeCheckpointHandler.write_tree_metadata default value is already set to True.\n",
      "Epochs: 100%|████████████████████████████████████| 100/100 [00:20<00:00,  4.81it/s, val_loss=0.0632]\n",
      "INFO | 16:27:16 | Finished training\n"
     ]
    }
   ],
   "source": [
    "!apax train config_cc_baseline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DFT -> CC Fine Tuning\n",
    "\n",
    "Finally, we can fine tune a model that was pretrained on DFT data.\n",
    "The model architecture remains unchanged for all 3 runs.\n",
    "However, for fine-tuning we need to specify the path to the base model and how to deal with its parameters.\n",
    "For each parameter group we can choose to freeze, to reset it or to keep training it.\n",
    "It is certainly advisable to experiment with different strategies, but a good start consists in freezing the embedding layer if the system we transfer to remains the same and resetting the scale-shift layer if the level of theory changes (DFT and CC have different energy scales).\n",
    "\n",
    "Make sure to carefully inspect the config options below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(\"config.yaml\")\n",
    "\n",
    "config_updates = {\n",
    "    \"n_epochs\": 100,\n",
    "    \"data\": {\n",
    "        \"n_train\": 50,\n",
    "        \"n_valid\": 10,\n",
    "        \"batch_size\": 4,\n",
    "        \"valid_batch_size\": 10,\n",
    "        \"experiment\": \"benzene_cc_ft\",\n",
    "        \"directory\": \"project/models\",\n",
    "        \"data_path\": str(cc_file_path),\n",
    "        \"energy_unit\": \"kcal/mol\",\n",
    "        \"pos_unit\": \"Ang\",\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"emb_lr\": 0.00,  # freeze embedding layer\n",
    "        \"nn_lr\": 0.0005,  # lower lr\n",
    "        \"scale_lr\": 0.001,  # lower lr\n",
    "        \"shift_lr\": 0.005,  # lower lr\n",
    "    },\n",
    "    \"checkpoints\": {\n",
    "        \"base_model_checkpoint\": \"project/models/benzene_dft\",  # pretrained model\n",
    "        \"reset_layers\": [\"scale_shift\"],  # reset scale-shift layer\n",
    "    },\n",
    "}\n",
    "config_dict = mod_config(config_path, config_updates)\n",
    "\n",
    "with open(\"config_cc_ft.yaml\", \"w\") as conf:\n",
    "    yaml.dump(config_dict, conf, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO | 16:27:20 | Running on [cuda(id=0)]\n",
      "INFO | 16:27:20 | Initializing Callbacks\n",
      "INFO | 16:27:20 | Initializing Loss Function\n",
      "INFO | 16:27:20 | Initializing Metrics\n",
      "INFO | 16:27:20 | Running Input Pipeline\n",
      "INFO | 16:27:20 | Read data file project/benzene_ccsd_t-train_mod.xyz\n",
      "INFO | 16:27:20 | Loading data from project/benzene_ccsd_t-train_mod.xyz\n",
      "INFO | 16:27:20 | Computing per element energy regression.\n",
      "INFO | 16:27:20 | Initializing Model\n",
      "INFO | 16:27:20 | initializing 1 models\n",
      "INFO | 16:27:24 | Initializing Optimizer\n",
      "INFO | 16:27:24 | loading checkpoint from project/models/benzene_dft/best\n",
      "INFO | 16:27:24 | Transferring parameters from project/models/benzene_dft\n",
      "INFO | 16:27:24 | Transferring parameter: radial_fn\n",
      "INFO | 16:27:24 | Transferring parameter: dense_0\n",
      "INFO | 16:27:24 | Transferring parameter: dense_0\n",
      "INFO | 16:27:24 | Transferring parameter: dense_1\n",
      "INFO | 16:27:24 | Transferring parameter: dense_1\n",
      "INFO | 16:27:24 | Transferring parameter: dense_2\n",
      "INFO | 16:27:24 | Transferring parameter: dense_2\n",
      "INFO | 16:27:24 | Beginning Training\n",
      "Epochs:   0%|                                                               | 0/100 [00:00<?, ?it/s]WARNING | 16:27:36 | SaveArgs.aggregate is deprecated, please use custom TypeHandler (https://orbax.readthedocs.io/en/latest/custom_handlers.html#typehandler) or contact Orbax team to migrate before May 1st, 2024. If your Pytree has empty ([], {}, None) values then use PyTreeCheckpointHandler(..., write_tree_metadata=True, ...) or use StandardCheckpointHandler to avoid TypeHandler Registry error. Please note that PyTreeCheckpointHandler.write_tree_metadata default value is already set to True.\n",
      "Epochs: 100%|███████████████████████████████████| 100/100 [00:20<00:00,  4.91it/s, val_loss=0.00483]\n",
      "INFO | 16:27:45 | Finished training\n"
     ]
    }
   ],
   "source": [
    "!apax train config_cc_ft.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the fine-tuned model achieves a lower validation loss than the baseline CC model.\n",
    "\n",
    "How much further can you improve the fine-tuning (or pretraining) setup?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
